{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1410586,"sourceType":"datasetVersion","datasetId":825073}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\nDATA_PATH = \"/kaggle/input/google-speech-commands\"\n\nSAMPLE_RATE = 16000       \nDURATION = 1.0           \nNUM_SAMPLES = int(SAMPLE_RATE * DURATION) \n\n# class RawAudioDataset(Dataset):\n#     def __init__(self, file_paths, labels):\n#         self.file_paths = file_paths\n#         self.labels = labels\n\n#     def __len__(self):\n#         return len(self.labels)\n\n#     def __getitem__(self, idx):\n#         waveform, _ = torchaudio.load(self.file_paths[idx])\n#         waveform = waveform[:, :NUM_SAMPLES]  # Trim or pad\n#         if waveform.shape[1] < NUM_SAMPLES:\n#             pad = NUM_SAMPLES - waveform.shape[1]\n#             waveform = torch.nn.functional.pad(waveform, (0, pad))\n\n#         waveform = waveform.unsqueeze(0)  # shape: (1, num_samples)\n#         label = self.labels[idx]\n#         return waveform, label\n\nclass RawAudioDataset(Dataset):\n    def __init__(self, file_paths, labels):\n        self.file_paths = file_paths\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        waveform, _ = torchaudio.load(self.file_paths[idx])\n        waveform = waveform[:, :NUM_SAMPLES]  # Trim or pad\n        if waveform.shape[1] < NUM_SAMPLES:\n            pad = NUM_SAMPLES - waveform.shape[1]\n            waveform = torch.nn.functional.pad(waveform, (0, pad))\n\n        \n        return waveform, self.labels[idx]\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:41:39.067010Z","iopub.execute_input":"2025-05-08T05:41:39.067470Z","iopub.status.idle":"2025-05-08T05:41:39.074745Z","shell.execute_reply.started":"2025-05-08T05:41:39.067445Z","shell.execute_reply":"2025-05-08T05:41:39.074104Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def create_balanced_dataset(data_path):\n    file_paths, labels = [], []\n    classes = sorted([\n        d for d in os.listdir(data_path)\n        if os.path.isdir(os.path.join(data_path, d)) and d != \"_background_noise_\"\n    ])\n    label_map = {label: idx for idx, label in enumerate(classes)}\n\n    class_to_files = {label: [] for label in classes}\n    for label in classes:\n        files = [f for f in os.listdir(os.path.join(data_path, label)) if f.endswith('.wav')]\n        class_to_files[label] = [os.path.join(data_path, label, f) for f in files]\n\n    min_class_count = min(len(files) for files in class_to_files.values())\n    min_class_count = 500\n\n    for label in classes:\n        selected_files = class_to_files[label][:min_class_count]\n        for file_path in selected_files:\n            file_paths.append(file_path)\n            labels.append(label_map[label])\n\n    return file_paths, labels, label_map\n\nfile_paths, labels, label_map = create_balanced_dataset(DATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:59:58.906782Z","iopub.execute_input":"2025-05-08T05:59:58.907515Z","iopub.status.idle":"2025-05-08T05:59:59.026173Z","shell.execute_reply.started":"2025-05-08T05:59:58.907488Z","shell.execute_reply":"2025-05-08T05:59:59.025491Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n\ntrain_dataset = RawAudioDataset(X_train, y_train)\ntest_dataset = RawAudioDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:02:51.526303Z","iopub.execute_input":"2025-05-08T06:02:51.526911Z","iopub.status.idle":"2025-05-08T06:02:51.538711Z","shell.execute_reply.started":"2025-05-08T06:02:51.526885Z","shell.execute_reply":"2025-05-08T06:02:51.537981Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        # x: (batch, time, d_model)\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nclass CNNLSTMTransformer(nn.Module):\n    def __init__(self, num_classes=10, input_channels=1, cnn_out_channels=128, lstm_hidden=128,\n                 transformer_heads=4, transformer_ff_dim=256, transformer_layers=2, sample_rate=16000):\n        super().__init__()\n\n        # 1D CNN for raw audio\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_channels, 64, kernel_size=80, stride=4, padding=38),  # ~conv1 from wav2vec\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Conv1d(64, cnn_out_channels, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(cnn_out_channels),\n        )\n\n        self.embedding_dim = cnn_out_channels\n\n        # LSTM\n        self.lstm = nn.LSTM(\n            input_size=self.embedding_dim,\n            hidden_size=lstm_hidden,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        )\n        lstm_output_dim = lstm_hidden * 2\n\n        # Positional Encoding\n        self.pos_encoder = PositionalEncoding(lstm_output_dim)\n\n        # Transformer Encoder\n        transformer_layer = nn.TransformerEncoderLayer(\n            d_model=lstm_output_dim,\n            nhead=transformer_heads,\n            dim_feedforward=transformer_ff_dim,\n            dropout=0.3,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=transformer_layers)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            #nn.LayerNorm(lstm_output_dim),\n            nn.Dropout(0.3),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(lstm_output_dim, num_classes)\n        )\n\n    def forward(self, x):\n        \n        x = self.cnn(x)  \n        x = x.permute(0, 2, 1)  \n    \n        x, _ = self.lstm(x)  \n        x = self.pos_encoder(x)  \n    \n        x = self.transformer(x)  \n    \n       \n        x = x.permute(0, 2, 1)  \n        x = self.classifier(x)\n        return x\n\n\n\n\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNNLSTMTransformer(num_classes=len(label_map)).to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:02:55.658638Z","iopub.execute_input":"2025-05-08T06:02:55.659238Z","iopub.status.idle":"2025-05-08T06:02:55.697557Z","shell.execute_reply.started":"2025-05-08T06:02:55.659212Z","shell.execute_reply":"2025-05-08T06:02:55.696753Z"}},"outputs":[{"name":"stdout","text":"CNNLSTMTransformer(\n  (cnn): Sequential(\n    (0): Conv1d(1, 64, kernel_size=(80,), stride=(4,), padding=(38,))\n    (1): ReLU()\n    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n    (4): ReLU()\n    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n  (pos_encoder): PositionalEncoding()\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.3, inplace=False)\n        (linear2): Linear(in_features=256, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.3, inplace=False)\n        (dropout2): Dropout(p=0.3, inplace=False)\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.3, inplace=False)\n    (1): AdaptiveAvgPool1d(output_size=1)\n    (2): Flatten(start_dim=1, end_dim=-1)\n    (3): Linear(in_features=256, out_features=30, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\ndef train(model, train_loader, criterion, optimizer, device, num_epochs=10):\n    model.train()\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        correct = 0\n        total = 0\n\n        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n\n        for batch in loop:\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Calculate accuracy\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            total_loss += loss.item()\n\n            # Update loop message\n            loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n\n        print(f'Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}, Accuracy = {100 * correct / total:.2f}%')\n\n# Train the model\nnum_epochs = 10\ntrain(model, train_loader, criterion, optimizer, device, num_epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:03:02.003201Z","iopub.execute_input":"2025-05-08T06:03:02.003798Z","iopub.status.idle":"2025-05-08T06:57:28.397380Z","shell.execute_reply.started":"2025-05-08T06:03:02.003776Z","shell.execute_reply":"2025-05-08T06:57:28.396674Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 188/188 [06:09<00:00,  1.97s/it, accuracy=24.5, loss=1.48]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 2.5559, Accuracy = 24.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 188/188 [05:23<00:00,  1.72s/it, accuracy=54.8, loss=1.2]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 1.4968, Accuracy = 54.82%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 188/188 [05:22<00:00,  1.72s/it, accuracy=65.3, loss=1.03] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 1.1241, Accuracy = 65.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 188/188 [05:22<00:00,  1.71s/it, accuracy=72.8, loss=0.711]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 0.8717, Accuracy = 72.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 188/188 [05:21<00:00,  1.71s/it, accuracy=76.6, loss=0.939]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 0.7463, Accuracy = 76.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 188/188 [05:20<00:00,  1.71s/it, accuracy=78.7, loss=0.325]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss = 0.6797, Accuracy = 78.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 188/188 [05:20<00:00,  1.70s/it, accuracy=83, loss=0.52]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss = 0.5433, Accuracy = 83.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 188/188 [05:21<00:00,  1.71s/it, accuracy=84.6, loss=0.506]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss = 0.4840, Accuracy = 84.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 188/188 [05:22<00:00,  1.71s/it, accuracy=86, loss=0.497]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss = 0.4452, Accuracy = 85.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 188/188 [05:22<00:00,  1.71s/it, accuracy=87.2, loss=0.376]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss = 0.4019, Accuracy = 87.21%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\ndef train(model, train_loader, criterion, optimizer, device, num_epochs=10):\n    model.train()\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        correct = 0\n        total = 0\n\n        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n\n        for batch in loop:\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Calculate accuracy\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            total_loss += loss.item()\n\n            # Update loop message\n            loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n\n        print(f'Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}, Accuracy = {100 * correct / total:.2f}%')\n\n# Train the model\nnum_epochs = 5\ntrain(model, train_loader, criterion, optimizer, device, num_epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T07:31:51.366645Z","iopub.execute_input":"2025-05-08T07:31:51.366956Z","iopub.status.idle":"2025-05-08T07:58:42.753742Z","shell.execute_reply.started":"2025-05-08T07:31:51.366932Z","shell.execute_reply":"2025-05-08T07:58:42.753003Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 188/188 [05:22<00:00,  1.72s/it, accuracy=89.1, loss=0.281]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 0.3275, Accuracy = 89.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 188/188 [05:22<00:00,  1.72s/it, accuracy=91.8, loss=0.466] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 0.2528, Accuracy = 91.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 188/188 [05:22<00:00,  1.71s/it, accuracy=91, loss=0.282]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 0.2773, Accuracy = 91.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 188/188 [05:21<00:00,  1.71s/it, accuracy=90.8, loss=0.151] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 0.2833, Accuracy = 90.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 188/188 [05:22<00:00,  1.71s/it, accuracy=92.3, loss=0.401] ","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 0.2312, Accuracy = 92.31%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"def evaluate(model, data_loader, criterion, device):\n    \"\"\"Evaluate the model\"\"\"\n    model.eval()\n\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    loop = tqdm(data_loader, desc=\"Evaluating\", leave=True)\n\n    for batch in loop:\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Transpose inputs for transformer\n        #inputs = inputs.transpose(1, 2)\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        total_loss += loss.item()\n        \n        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n\n    accuracy = 100 * correct / total\n    avg_loss = total_loss / len(data_loader)\n    \n    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\nevaluate(model, test_loader, criterion, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:02:01.886266Z","iopub.execute_input":"2025-05-08T09:02:01.886534Z","iopub.status.idle":"2025-05-08T09:02:22.970637Z","shell.execute_reply.started":"2025-05-08T09:02:01.886515Z","shell.execute_reply":"2025-05-08T09:02:22.969780Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 47/47 [00:21<00:00,  2.23it/s, accuracy=84.5, loss=0.99] ","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7192, Accuracy: 84.47%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"torch.save(model.state_dict(), \"cnn_lstm_transformer_model1.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:42:18.544529Z","iopub.execute_input":"2025-05-08T08:42:18.544795Z","iopub.status.idle":"2025-05-08T08:42:18.569059Z","shell.execute_reply.started":"2025-05-08T08:42:18.544777Z","shell.execute_reply":"2025-05-08T08:42:18.568414Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:22:13.870580Z","iopub.execute_input":"2025-05-08T08:22:13.871119Z","iopub.status.idle":"2025-05-08T08:22:14.127272Z","shell.execute_reply.started":"2025-05-08T08:22:13.871088Z","shell.execute_reply":"2025-05-08T08:22:14.126580Z"}},"outputs":[],"execution_count":53}]}